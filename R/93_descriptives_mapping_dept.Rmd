---
title: "Appendix B: Luxembourg Research Institute Evaluation 2022: Mapping of Knowledge Structure: Department internal"
author: "Daniel S. Hain"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   bookdown::pdf_document2: 
    fig_caption: true
    number_sections: true
    global_numbering: true
params:
    institute: 
       value: null
    department:
       value: null
---

<!---
# Add to YAML when compiling html --< when reviewing change first line to: html_notebook
  html_document:
    theme: flatly
    code_folding: hide
    df_print: paged
    number_sections: true
    toc: false
    toc_depth: 3
    toc_float:
      collapsed: false

# Add when compiling pdf
   bookdown::pdf_document2: 
    fig_caption: true
    number_sections: true
    global_numbering: true
--->


```{=html}
<style type="text/css">
.main-container {
  max-width: 1200px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE}
### Generic preamble
#rm(list=ls())
Sys.setenv(LANG = "en")
options(scipen = 5)
set.seed(1337)

### Load packages  
# general
library(tidyverse)
library(magrittr)

# Kiblio & NW
library(bibliometrix)
library(tidygraph)
library(ggraph)

# NLP
library(tidytext)

# Dataviz
library(plotly)
library(ggforce)
library(ggrepel)
library(patchwork)

# Knit
library(knitr) # For display of the markdown
library(kableExtra) # For table styling

# own functions
source("../functions/functions_basic.R")
source("../functions/functions_summary.R")
source("../functions/00_parameters.R")

# Knitr options
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
```

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

```{r, include=FALSE}
#var_inst <- 'LIST'
#var_dept <- 'ERIN'
```

```{r, include=FALSE}
var_inst <- params$institute
var_dept <- params$department
```

# Introduction: `r var_inst` Department `r var_dept`

## Purpose & Structure

This document provides additional analysis and visualizations regarding the Luxembourg Research Institute Evaluation 2022 of  `r var_inst` Department `r var_dept`. Its purpose is:

* To map the broader research community and distinct research field the department contributes to.
* Identify core knowledge bases, research areas gtrends and topics.
* Highlight the positioning of the department within this dynamics.

It is structured as follows:

1. **Topics:** Further information regarding the departments research topics and their development
2. **Knowledge Bases:** Further information regarding the departments knowledge bases and their development
3. **Research Areas:** Further information regarding the departments research areas and their development
4. **KB-RA-TP interaction:** Joint visualization of knowledge bases, research areas, and topics.
5. **Specialization:** Departments research area and topic specialization compared to the broader research field.
6. **Collaboration:** Co-Authorship networks to other research institutions.
7. **TEchnical description:** Further guidance regarding concepts, methods, indicators, techniques, and workflows.

## Further resources

This document and further factlitating information can be found online under:

* This document: `r paste0('https://daniel-hain.github.io/biblio_lux_2022/output/field_mapping/field_mapping_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.html')`
* Mapping of broad research field: `r paste0('https://daniel-hain.github.io/field_mapping/biblio_lux_2022/output/field_mapping_dept_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.html')`
* Interactive Topic Modelling: `r paste0('https://daniel-hain.github.io/biblio_lux_2022/output/topic_modelling/LDAviz_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds/index.html#topic=1&lambda=0.60&term=')`
* The method for the research-field-mapping can be reviewed here: [Rakas, M., & Hain, D. S. (2019). The state of innovation system research: What happens beneath the surface?. Research Policy, 48(9), 103787.](https://doi.org/10.1016/j.respol.2019.04.011)


```{r, include=FALSE}
# Load data
M_all <- readRDS(paste0('../../temp/M_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %>% as_tibble() %>% 
  distinct(UT, .keep_all = TRUE) %>% 
  mutate(PY = as.numeric(PY),
         period = ifelse(PY <= 2018, 1, 2)) %>%
  filter(PY >= PY_min, PY <= PY_max) %>%
  group_by(PY) %>%
    mutate(TC_cohort_rank = percent_rank(TC)) %>%
  ungroup() 
```

```{r}
# Filter for department only.
M <- M_all %>% filter(int_dept == TRUE)
```

```{r}
com_labels <- read_csv2('../../data/community_labeling.csv')  %>% filter(institute == var_inst, department == var_dept) %>% arrange(institute, department, type, com) %>% mutate(label = ifelse(is.na(label), paste0(type, ' ', com, ': unlabeled'), paste0(type, ' ', com, ': ', label)))
```

# Topic modelling

```{r, include=FALSE}
text_tidy <- readRDS(paste0('../../temp/text_tidy_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))
text_lda <- readRDS(paste0('../../temp/text_LDA_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) 

text_lda_beta <- text_lda %>% tidy(matrix = "beta") 
text_lda_gamma <- text_lda %>% tidy(matrix = "gamma")
```

```{r, include=FALSE}
com_names_top <- tibble( 
  com = 1:(text_lda_gamma %>% pull(topic) %>% n_distinct()),
  type = 'TP',
  col = com %>% gg_color_select(pal = pal_tp)) %>%
  left_join(com_labels %>% filter(type == 'TP') %>% select(com, label), by = 'com') %>%
  mutate(label = ifelse(is.na(label), paste0('TP ', com, ': unlabeled'), label))
            
# # 1st alternative: Number them 1-n
# paste(type, 1:(text_lda_gamma %>% pull(topic) %>% n_distinct()))           
```

```{r, include=FALSE}
text_lda_beta %<>%  inner_join(com_names_top %>% select(com, label, col), by = c('topic' = 'com'))
text_lda_gamma %<>% inner_join(com_names_top %>% select(com, label, col), by = c('topic' = 'com'))
```

## Topic Development

Shows changes in topical focus of the department over time.

```{r, fig.width = 15, fig.height=7.5}
text_lda_gamma %>%
  rename(weight = gamma) %>%
  inner_join(M %>% select(XX, PY) %>%
              filter(int_dept = TRUE), by = c('document' = 'XX')) %>%
  mutate(PY = as.numeric(PY)) %>%
  group_by(PY, label) %>% summarise(weight = sum(weight)) %>% ungroup() %>%
  group_by(PY) %>% mutate(weight_PY = sum(weight)) %>% ungroup() %>%
  mutate(weight_rel = weight / weight_PY) %>%
  select(PY, label, weight, weight_rel) %>%
  filter(PY >= PY_min & PY <= PY_max) %>%
  arrange(PY, label) %>%
  plot_summary_timeline(y1 = weight, y2 = weight_rel, t = PY, t_min = PY_min, t_max = PY_max, by = label,  label = TRUE, pal = pal_tp, y1_text = "Topic popularity annualy", y2_text = "Share of topic annually") +
  plot_annotation(title = paste('Topic Modelling: Research Field of', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Absolute topic appearance (left), Relative topic appearance (right)')
```

## Topic Ranking

Shows changes in topical focus of the department over time (ranking).

```{r, fig.width=15, fig.height=7.5}
text_lda_gamma %>%
  rename(weight = gamma) %>%
  inner_join(M %>% select(XX, PY), by = c('document' = 'XX')) %>%
  mutate(PY = as.numeric(PY)) %>%
  group_by(PY, label) %>% summarise(weight = sum(weight)) %>% ungroup() %>%
  group_by(PY) %>% mutate(rank = max(rank(weight)) - rank(weight) + 1) %>% ungroup() %>%
  ggplot(aes(x = PY, y = rank, group = label, col = label)) +
  geom_line(size = 2) +
  geom_point(size = 4) +
  scale_y_reverse(breaks = 1:nrow(com_names_top)) +
  theme(legend.position = 'bottom') + 
  labs(title = paste('Topic Modelling:', var_inst, 'Dept.', var_dept, sep = ' '),
       subtitle = 'Rank of topic prominence in department publications',
       caption = '1 = Most prominent topic',
       x = 'Year',
       y = 'Rank',
       color = '')
```
## Topic Impact

Shows which topics where particularly impactful (cohort citation percentile) over time.

```{r, fig.width = 15, fig.height=7.5}
text_lda_gamma %>%
  rename(weight = gamma) %>%
  inner_join(M %>% select(XX, PY, TC_cohort_rank) %>%
              filter(int_dept = TRUE), by = c('document' = 'XX')) %>%
  # mutate(impact = weight* TC_cohort_rank) %>%
  mutate(impact = TC_cohort_rank >= 0.9) %>%
  group_by(PY) %>%
  filter(percent_rank(weight) >= 0.5) %>%
  ungroup() %>%
  group_by(label, PY) %>%
  summarise(impact = mean(impact)) %>%
  ggplot(aes(x = PY, y = impact, col = label)) +
  geom_line(size = 2) +
  geom_point(size = 4) +
  theme(legend.position = 'bottom') +
  labs(title = paste('Topic Impact:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  x = 'Year',
                  y = 'Topic Impact',
                  caption = 'Topic Impact refers to the share of publications within the cohort top-10% cited publications associated with the topic.',
                  )
```




<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

```{r, include=FALSE}
rm(text_tidy, text_lda)
```


# Knowledge Bases: Co-Citation network analysis 

```{r, include=FALSE}
C_nw <- readRDS(paste0('../../temp/C_nw_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))  %>%
  drop_na(com)
```

```{r, include=FALSE}
com_names_cit <- tibble( 
  com = 1:(C_nw %>% pull(com) %>% n_distinct()),
  type = 'KB',
  col = com %>% gg_color_select(pal = pal_kb)) %>%
  left_join(com_labels %>% filter(type == 'KB') %>% select(com, label), by = 'com') %>%
  mutate(label = ifelse(is.na(label), paste0('KB ', com, ': unlabeled'), label))

# # 1st alternative: Number them 1-n
# paste(type, 1:(C_nw %>% pull(com) %>% n_distinct()))
```

```{r, include=FALSE}
C_nw %<>% left_join(com_names_cit %>% select(com, label, col), by = "com")
```

```{r, include=FALSE}
el_2m <- readRDS(paste0('../../temp/el_2m_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %>%
  drop_na()
```


```{r, include=FALSE}
cit_com_year <- el_2m %>%
  count(com_cit, PY, name = 'TC') %>%
  group_by(PY) %>%
  mutate(TC_rel = TC / sum(TC)) %>%
  ungroup() %>%
  arrange(PY, com_cit) %>%
  left_join(com_names_cit , by = c('com_cit' = 'com')) %>% 
  complete(label, PY, fill = list(TC = 0, TC_rel = 0))
```

## Knowledge Base development

Shows the development of citations to the knowledge bases by department publications.

```{r, fig.width = 15, fig.height=7.5}
cit_com_year %>%
  plot_summary_timeline(y1 = TC, y2 = TC_rel, t = PY, t_min = PY_min, t_max = PY_max, by = label, pal = pal_kb, label = TRUE,
                        y1_text = "Number citations recieved annually",  y2_text = "Share of citations recieved annually") +
  plot_annotation(title = paste('Knowledge Bses:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Absolute knowledge base appearance (left), Relative knowledge base appearance (right)')
```

# Research Areas: Bibliographic coupling analysis 

```{r, include=FALSE}
M_bib_all <- readRDS(paste0('../../temp/M_bib_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %>% 
  as_tibble() %>%
  drop_na(com)
```

```{r, include=FALSE}
# Filter for department only.
M_bib <- M_bib_all %>% semi_join(M, by = 'XX')
```

```{r, include=FALSE}
com_names_bib <- tibble( 
  com = 1:(M_bib %>% pull(com) %>% n_distinct()),
  type = 'RA',
  col = com %>% gg_color_select(pal = pal_ra)) %>%
  left_join(com_labels %>% filter(type == 'RA') %>% select(com, label), by = 'com') %>%
  mutate(label = ifelse(is.na(label), paste0('RA ', com, ': unlabeled'), label))
```

```{r, include=FALSE}
M_bib %<>% left_join(com_names_bib %>% select(com, label, col), by = "com")
```

## Research Area Development

Shows development of department publications in the research areas over time.

```{r, fig.width = 15, fig.height=7.5}
M_bib %>%
  left_join(M %>% select(XX, PY), by = 'XX') %>%
  mutate(PY = PY %>% as.numeric()) %>%
  group_by(label, PY) %>% summarise(n = n()) %>% ungroup() %>%
  group_by(PY) %>% mutate(n_PY = sum(n)) %>% ungroup() %>%
  mutate(n_rel = n / n_PY) %>%
  select(label, PY, n, n_rel) %>%
  arrange(label, PY) %>% 
  complete(label, PY, fill = list(n = 0, n_rel = 0)) %>%
  plot_summary_timeline(y1 = n, y2 = n_rel, t = PY, t_min = PY_min, t_max = PY_max, by = label, label = TRUE, pal = pal_ra,
                        y1_text = "Number publications annually", y2_text = "Share of publications annually") +
  plot_annotation(title = paste('Research Areas:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Absolute research area appearance (left), Relative research area appearance (right)')
```

```{r, include=FALSE}
g_agg <- readRDS(paste0('../../temp/g_bib_agg_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %N>% 
  left_join(com_names_bib, by = 'com') %N>%
  mutate(label = label %>% factor()) %E>%
  mutate(type = 'Knowledge Base Similarity')
#   mutate(name = names_ra %>% pull(com_ra_name),
#          color = cols_ra)

## TODO: INclude topic similarity
## TODO: INclude topic similarity
## TODO: INclude topic similarity
```

## Research Area Similarity

Shows the similarity of the departments research areas ()by bibliographic coupling strenght).

```{r, fig.height= 7.5, fig.width=7.5}
g_agg %E>% 
  filter(weight > 0 & from != to) %>%
  filter(weight >= quantile(weight, 0.25) )  %>%
  ggraph(layout = "circle") + 
  geom_edge_fan(strenght = 0.8, aes(width = weight), alpha = 0.2)  + 
  geom_node_point(aes(size = N, color = label))  + 
  geom_node_text(aes(label = label), repel = TRUE) +
  #theme_graph(base_family = "Arial") +
  theme(legend.position = 'bottom') +
  scale_size(range = c(2,20)) +
  scale_color_brewer(palette = pal_ra) +
  labs(title = paste('Research Area Similarity:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Nodes = Identified Research Areas; Edges: Bibliographic coupling strenght (Jaccard weighted)')
```

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Knowledge Bases, Research Areas & Topics Interaction

```{r, include=FALSE}
# Nodes
nl_3m <- com_names_bib %>%
  bind_rows(com_names_cit) %>%
  bind_rows(com_names_top) %>%
  rename(name = label,
         com_nr = com) %>%
  relocate(name)

# Edges
el_2m_kb <- el_2m %>%
  select(-from, -to) %>%
  inner_join(com_names_cit %>% select(com, label), by = c('com_cit' = 'com')) %>%
  inner_join(com_names_bib %>% select(com, label, col), by = c('com_bib' = 'com')) %>%
  mutate(weight = 1) %>%
  rename(from = label.x,
         to = label.y) %>% # generic
  select(from, to, weight, col, PY) %>% 
  drop_na() %>% 
  mutate(period = ifelse(PY <= 2018, 1, 2)) %>%
  count(from, to, col, period, wt = weight, name = 'weight') %>%
  filter(percent_rank(weight) >= 0.25) %>%
  weight_jaccard(i = from, j = to, w = weight) %>% 
  select(-weight)

el_2m_topic <- text_lda_gamma %>% select(-topic, -col) %>%
  left_join(M_bib %>% select(XX, com) %>% drop_na(com), by = c('document' = 'XX')) %>%
  inner_join(com_names_bib %>% select(com, label, col), by = c('com' = 'com')) %>%
  rename(from = label.y,
         to = label.x,
         weight = gamma) %>% # generic
  select(from, to, weight, col) %>% 
  drop_na() %>% 
  count(from, to, col, wt = weight, name = 'weight') %>%
  filter(percent_rank(weight) >= 0.25) %>%
  weight_jaccard(i = from, j = to, w = weight) %>% select(-weight)

# graph
g_3m <- el_2m_kb %>% 
  bind_rows(el_2m_topic) %>%
  as_tbl_graph(directed = TRUE) %N>%
  left_join(nl_3m, by = 'name') %>%
  mutate(
    level = case_when(
      type == "KB" ~ 1,
      type == "RA" ~ 2,
      type == "TP" ~ 3),
    coord_y = 0.1,
    coord_x = 0.001 + 1/(max(level)-1) * (level-1)
    )  %N>%
  filter(!is.na(level))
```

```{r, include=FALSE}
## Build sankey plot
fig <- plot_ly(type = "sankey", 
               orientation = "h",
               arrangement = "snap",
  node = list(
    label = g_3m %N>% as_tibble() %>% pull(name),
    x = g_3m %N>% as_tibble() %>% pull(coord_x),
    y = g_3m %N>% as_tibble() %>% pull(coord_y),
    color = g_3m %N>% as_tibble() %>% pull(col), 
    pad = 4
  ), 
  link = list(
    source = (g_3m %E>% as_tibble() %>% pull(from)) -1,
    target = (g_3m %E>% as_tibble() %>% pull(to)) -1,
    value =  g_3m %E>% as_tibble() %>% pull(weight_jac),
    color = g_3m %E>% as_tibble() %>% pull(col) %>% col2rgb() %>% as.matrix() %>% t() %>% as_tibble() %>% 
      mutate(col_rgb = paste0('rgba(', red, ',' , green, ',', blue, ',0.75)')) %>%  pull(col_rgb)
    )
) %>% 
  layout(title = paste('Knowledge Bases, Research Areas & Topics:', var_inst, 'Dept.', var_dept, sep = ' '),
         margin = list(l = 50, r = 50, b = 100, t = 100, pad = 2)) 
```

## Joint Overview over Knowledge Bases, Research Areas, and Topics

This plot shows the connection of publications in the research areas to knowledge bases (by citations) and topics (by gamma, document-topic weight)

```{r, fig.height= 10, fig.width=15}
fig
```



# Trends 

```{r}
uni_sim <- read_rds(paste0('../../temp/uni_sim_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))
```

## Lead-Lag Analysis 

```{r, fig.height=5, fig.width=7.5}
desc <- uni_sim %>% 
  filter(AU_UN == paste(var_inst, var_dept)) %>%
  mutate(desc = paste0('N: ', n, ' \n',
                       'Similarity to past:', sim_past %>% round(2), ' \n',
                       'Similarity to future: ', sim_future %>% round(2), ' \n',
                       'Lead-Lag Score: ', future_trend %>% round(2))) %>% pull(desc)
  
  
uni_sim %>%
  slice_max(order_by = n, n = 100) %>%
  ggplot(aes(x = sim_past, y = sim_future)) +
  geom_mark_ellipse(aes(filter = AU_UN == paste(var_inst, var_dept), label = AU_UN, fill = 'red1'), description = desc) + guides(fill = FALSE) +
  #geom_point(data = uni_sim %>% filter(AU_UN == paste(var_inst, var_dept)), 
  #           aes(size = n), col = 'darkred') + 
  geom_point(aes(size = n, col = future_trend), alpha = 0.5) +
  geom_text_repel( 
    data =uni_sim %>% slice_max(order_by = n, n = 5) %>% filter(AU_UN != paste(var_inst, var_dept)), # Filter data first
    aes(label = AU_UN),
    box.padding = 1, max.overlaps = Inf) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", col = 'snow3') +
  scale_colour_gradient(low = "blue", high = "red", name = "Lead-Lag Score") +
  theme(legend.position = 'bottom') +
  labs(x = 'Similarity to past',
       y = 'Similarity to future',
       title = paste('Field Lead-Lag Analysis:', var_inst, 'Dept.', var_dept, sep = ' '),
       subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
       caption = 'x and y axis')
# + lims(x = c(0.625, 0.725), y = c(0.625, 0.725))
```

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Collaboration 

```{r}
el_inst <- readRDS(paste0('../../temp/el_inst_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))
```


```{r}
g_inst <- el_inst %>% as_tbl_graph(directed = FALSE) %E>%
  filter(weight >= cutof_edge_cit ) %N>%
  filter(!node_is_isolated())
```

## Collaboration network 2nd Degree

Shows overl collaboration (co-authorship) structure of the department.

```{r, fig.height= 10, fig.width=15}
g_inst %>%
  convert(to_local_neighborhood,
                     node = which(.N()$name == paste0(var_inst, ' ', var_dept)),
                     order = 2,
                     mode = "all") %N>% 
  mutate(cent = centrality_degree(weights = weight)) %>%
  filter(name == paste(var_inst, var_dept)  | rank(-cent) <= 100 ) %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(width = weight,
                     color = .N()$name[from] == paste(var_inst, var_dept) | .N()$name[from] == paste(var_inst, var_dept),
                     filter = (weight >= weight  %>% quantile(0.25)))
                 , alpha = 0.25)  + 
  geom_node_point(aes(size = cent), col = 'steelblue1')  + 
  geom_node_text(aes(label = name, 
                     size = cent,
                     filter = (cent >= cent  %>% quantile(0.8))), repel = TRUE) +
  #theme_graph(base_family = "Arial") +
  theme(legend.position = 'bottom') +
  scale_edge_width_continuous(range = c(0.5, 5))  +
  scale_edge_colour_manual(values = c("grey", "red"), name = '1st degree') +
  scale_size(range = c(0.5,10)) 
```

# Missing Publications

The bibliometric part of the 2022 research evaluation is based on Scopus data, one of the most comprehensive providers of academic publications. However, some potentially relevant scholarly output is not covered. Reasons might be that the publication outlet is not Scopus indexed. While most academic journals are, the coverage of bookchapters and conference proceedings is less exhaustive. In rare cases, errors in the database can also lead to underreporting.

To get an exhaustive overview covering all scholarly output, find below a breakdown of publications not matched with Scopus, and therefore not included in the overal evaluation. 

```{r, include=FALSE}
# load data

if(var_inst != 'LIST'){ 
  data <- read_tsv(paste0('../../data/publications_', str_to_lower(var_inst), '.txt')) 
  colnames(data) <-colnames(data) %>% str_to_lower() %>% str_replace_all(' ', '_') %>% str_remove_all('[^[:alnum:]_]')

# First cleaning & filtering
data %<>% 
  # Filter timeframe
  filter(year >= PY_min,
         year <= PY_max) %>%
  # clean up DOIs
   mutate(doi = dois_digital_object_identifiers %>% 
            str_remove('^.*doi.org/') %>% 
            str_remove('^.*dx\\.') %>% 
            str_remove('^/') %>% 
            str_remove(' ') %>% 
            str_squish() %>%
            str_replace_all('%', '/')) %>%
  # complete missing DOI
  group_by(pure_id) %>%
  arrange(pure_id, doi) %>%
  fill(doi, .direction = 'downup') %>%
  ungroup()

# ! Note: For matching with departments 2 different forkflows for the institutes
# filter institute & departments
if(var_inst == 'LISER'){ data %<>% rename(unit = organisations_of_contributors) }
if(var_inst == 'LIH'){ data %<>% rename(unit = parent_organisational_units) }

# Join with provided unit maps to get the correct joined units
data %<>% 
  inner_join(read_csv2('../../data/mapping_units.csv') %>% filter(institute == var_inst, !is.na(unit_short)) %>% select(unit_old, unit_short),
             by = c('unit' = 'unit_old')) 

# Join with provided unit maps to get the correct joined units
data %<>% 
  inner_join(read_csv2('../../data/mapping_units.csv') %>% filter(institute == var_inst, !is.na(unit_short)) %>% select(unit_old, unit_short),
             by = c('unit' = 'unit_old')) 

# load matching errors from SciVal (self-created c&p from scival import)
data %<>% 
  left_join(read_csv2('../output/publications/pub_doi_unmatched.csv') %>% filter(institute == var_inst, unit == var_dept) %>% select(doi, issue_scival), by = 'doi') %>%
  mutate(issue = if_else(is.na(doi), 'missing DOI', issue_scival)) %>%
  drop_na(issue)
}
```



## By Reason

```{r}
if(var_inst != 'LIST'){ 
data %>%
  count(issue, year) %>%
  ggplot(aes(x = year, y = n, fill = issue)) + 
  geom_col() +
  theme(legend.position = 'bottom') +
  labs(title = paste(var_inst, 'publications dept.', var_dept, 'not included in reporting', sep = ' '),
       subtitle = 'By reason',
       x = 'Year', y = 'Number of Publications')
}
```

## By Type

```{r}
if(var_inst != 'LIST'){ 
data %>%
  count(type, year) %>%
  ggplot(aes(x = year, y = n, fill = type)) + 
  geom_col() +
  theme(legend.position = 'bottom') +
  labs(title = paste(var_inst, 'publications dept.', var_dept, 'not included in reporting', sep = ' '),
       subtitle = 'By publication type',
       x = 'Year', y = 'Number of Publications')
}
```

## By outlet (Journal articles only)

```{r results='asis'}
if(var_inst != 'LIST'){ 
data %>%
  filter(type == 'Article') %>%
  mutate(journal_title = journal_title %>% str_replace_all("[^[:alnum:]]", " ") ) %>%
  count(journal_title, sort = TRUE) %>%
    kable(format = 'latex', booktabs = TRUE, caption = "Outlet of unmatched journal publication", position = '!ht')
    #kable(booktabs = TRUE, caption = "Outlet of unmatched journal publication", position = '!ht') 
}
```



<!---
# Technical description


## LDA Topic Modelling

Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA, [Blei et al., 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com)) is an example of topic model and is used to classify text in a document to a particular topic. 

LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.

### LDAVis

[LDAvis](https://github.com/cpsievert/LDAvis) is a web-based interactive visualisation of topics estimated using LDA ([Sievert & Shirley, 2014](https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)). It provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization. The visualisation has two basic pieces.

The **left panel** visualise the topics as circles in the two-dimensional plane whose centres are determined by computing the Jensen–Shannon divergence between topics, and then by using multidimensional scaling to project the inter-topic distances onto two dimensions. Each topic’s overall prevalence is encoded using the areas of the circles.

The **right panel** depicts a horizontal bar chart whose bars represent the individual terms that are the most useful for interpreting the currently selected topic on the left. A pair of overlaid bars represent both the corpus-wide frequency of a given term as well as the topic-specific frequency of the term.

The $\lambda$ slider allows to rank the terms according to term relevance. By default, the terms of a topic are ranked in decreasing order according their topic-specific probability ( $\lambda$ = 1 ). Moving the slider allows to adjust the rank of terms based on much discriminatory (or "relevant") are for the specific topic. The suggested optimal value of $\lambda$ is 0.6.

## Research Areas: 
In a bibliographic coupling network, the **coupling-strength** between publications is determined by the number of commonly cited references they share, assuming a common pool of references to indicate similarity in context, methods, or theory. Formally, the strength of the relationship between a publication pair $i$ and $j$ ($s_{i,j}^{bib}$) is expressed by the number of commonly cited references. 

$$s_{i,j}^{bib} = \sum_m c_{i,m} c_{j,m}$$

Since our corpus contains publications which differ strongly in terms of the number of cited references, we normalize the coupling strength by the Jaccard similarity coefficient. Here, we weight the intercept of two publications' bibliography (shared refeences) by their union (number of all references cited by either $i$ or $j$). It is bounded between zero and one, where one indicates the two publications to have an identical bibliography, and zero that they do not share any cited reference. Thereby, we prevent publications from having high coupling strength due to a large bibliography (e.g., literature surveys).

$$S_{i,j}^{jac-bib} =\frac{C(i \cap j)}{C(i \cup j)} = \frac{s_{i,j}^{bib}}{c_i + c_j - s_{i,j}^{bib}}$$

More recent articles have a higher pool of possible references to co-cite to, hence they are more likely to be coupled. Consequently, bibliographic coupling represents a forward looking measure, and the method of choice to identify the current knowledge frontier at the point of analysis.


## Knowledge Bases
In a co-cittion network, the strength of the relationship between a reference pair $m$ and $n$ ($s_{m,n}^{coc}$) is expressed by the number of publications $C$ which are jointly citing reference $m$ and $n$. 

$$s_{m,n}^{coc} = \sum_i c_{i,m} c_{i,n}$$

The intuition here is that references which are frequently cited together are likely to share commonalities in theory, topic, methodology, or context. It can be interpreted as a measure of similarity as evaluated by other researchers that decide to jointly cite both references. Because the publication process is time-consuming, co-citation is a backward-looking measure, which is appropriate to map the relationship between core literature of a field.

--->

```{r}
# After knitted do this
#file.rename(from = "92_descriptives_mapping.nb.html", to = paste0('../output/field_mapping/field_mapping_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.html'))
```





