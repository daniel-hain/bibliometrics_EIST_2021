---
title: "Transitions Bibliometrics 2020 - Exploring Embeddings"
author: "Daniel S. Hain"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook:
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    code_folding: hide
---

```{r setup, include=FALSE}
### Generic preamble
Sys.setenv(LANG = "en")
options(scipen = 5)
set.seed(1337)

### Load packages  
library(kableExtra) # For table styling
library(tidyverse)
library(magrittr)
```

# Load data

```{r}
M <- readRDS("../../temp/M.RDS") %>% as_tibble()

labels <- read_csv2('../data/tm_paper_concept_clean.csv')
```

```{r}
colnames(labels) <- colnames(labels) %>% str_to_lower() %>% str_remove_all('[\\,\\-\\.]')
```

```{r}
labels %<>% 
  select(-id_orig) %>%
  rename(XX = id_new) %>%
  drop_na(XX)
```

# Explore

```{r, fig.height=10}
labels %>%
  pivot_longer(-XX) %>%
  count(name, wt = value, sort = TRUE) %>%
  mutate(name = fct_reorder(name, n)) %>%
  ggplot(aes(x = name, y = n)) +
  geom_col() + 
  coord_flip()
```

```{r}
min_label = 50

label_select <- c(TRUE, colSums(labels %>% select(-XX)) >= min_label)
```

```{r}
labels <- labels[, label_select]
```

```{r}
data_text <- M %>% rename(text = AB) %>% 
  select(XX, text) %>%
  mutate(text = text %>% 
           str_to_lower() %>%
           str_replace_all("&", "-and-") %>%
           str_remove_all("/(&trade;|&reg;|&copy;|&#8482;|&#174;|&#169;)/.*") %>%
           iconv(to = "UTF-8", sub = "byte") %>%
           str_remove_all("ï¿½.*") %>%
           str_remove_all('[:digit:]') %>%
           str_squish()) %>% 
  drop_na()  
```


```{r}
data_text %>% write_csv('../../temp/data_text.csv')
labels%>% write_csv('../../temp/data_labels.csv')
```

# Embeddings

```{r}
library(text)
```


```{r}
# When padding is neccessary
#pad_fun <- function(x, n = 521) {
#  ul = unlist(strsplit(x, split = "\\s+"))[1:n]
#  paste(ul,collapse=" ") %>% str_remove_all(' NA')
#}

#data_text$text <- unlist(lapply(data_text$text, pad_fun))
```


```{r}
# Transform the text data to BERT word embeddings
embeddings <- data_text %>% 
  pull(text) %>% 
  textEmbed(model = 'bert-base-uncased') # 'allenai/scibert_scivocab_uncased'
```

```{r}
embeddings %>% saveRDS('../data/embeddings.rds')
```

```{r}
embeddings$text %>% head()
```

```{r}
library(uwot)
```

```{r}
res_umap <- embeddings$text %>%
  umap(metric = "cosine") 
```

```{r}
res_umap %>%
  as_tibble() %>%
  bind_cols(data_text %>% select(XX)) %>%
  inner_join(labels, by = 'XX') %>%
  ggplot(aes(x = V1, y = V2, col = tis %>% factor())) + 
  geom_point(shape = 21, alpha = 0.5) 
```

# Prediction

```{r}
library(keras)
```


```{r}
x_train <- embeddings$text %>% as.matrix()

y_train <- data_text %>% 
  select(XX) %>% 
  left_join(labels, by = 'XX') %>%
  select(-XX) %>%
  mutate(across(everything(), ~replace_na(., 0)))  %>%
  as.matrix()
```

```{r}

model1 <- keras_model_sequential() %>% 
  # architecture
  layer_dense(units = 1024, activation = "relu", input_shape = ncol(x_train)) %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = ncol(y_train), activation = 'sigmoid') 
```


```{r}
model1 %>%
  compile(optimizer = optimizer_adam(), 
          loss = loss_binary_crossentropy(), 
          metrics = list('accuracy'),
          )
```

```{r}
# set.seed(1337)
hist1 <- model1 %>% fit(x = x_train, 
                        y = y_train, 
                        validation_split = 0.20, 
                        epochs = 100,
                        callbacks = list(callback_tensorboard("logs/run_a"),
                                         callback_early_stopping(patience = 15))
                        )
```

```{r}
tensorboard("logs/run_a")
```



